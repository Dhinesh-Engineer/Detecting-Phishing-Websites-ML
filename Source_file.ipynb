{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLb3uSLKKhyt",
        "outputId": "24f141b9-d8cd-435f-b4a6-5b0f75448d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best classifier for detecting phishing websites.\n",
            "Accuracy score using Random forest is: 96.7662373252946\n",
            "\n",
            "Worst classifier for detecting phishing websites.\n",
            "Accuracy score using One Class SVM is: 48.09536859413538\n",
            "\n",
            "[-1, 0, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1]\n",
            "[-1]\n",
            "Not pHishing\n"
          ]
        }
      ],
      "source": [
        "import ipaddress\n",
        "import re\n",
        "import requests\n",
        "from datetime import date\n",
        "from dateutil.parser import parse as date_parse\n",
        "\n",
        "# Calculates number of months\n",
        "def diff_month(d1, d2):\n",
        "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
        "\n",
        "# Generate data set by extracting the features from the URL\n",
        "def generate_data_set(url):\n",
        "\n",
        "    data_set = []\n",
        "\n",
        "    # Converts the given URL into standard format\n",
        "    if not re.match(r\"^https?\", url):\n",
        "        url = \"http://\" + url\n",
        "    \n",
        "    # Stores the response of the given URL\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except:\n",
        "        response = \"\"\n",
        "\n",
        "    # Extracts domain from the given URL\n",
        "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
        "\n",
        "    # Requests all the information about the domain\n",
        "    whois_response = requests.get(\"https://www.whois.com/whois/\"+domain)\n",
        "\n",
        "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
        "        \"name\": domain\n",
        "    })\n",
        "\n",
        "    # Extracts global rank of the website\n",
        "    try:\n",
        "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
        "    except:\n",
        "        global_rank = -1\n",
        "\n",
        "    # having_IP_Address\n",
        "    try:\n",
        "        ipaddress.ip_address(url)\n",
        "        data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "    \n",
        "    # URL_Length\n",
        "    if len(url) < 54:\n",
        "        data_set.append(1)\n",
        "    elif len(url) >= 54 and len(url) <= 75:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "    \n",
        "    # Shortining_Service\n",
        "    if re.findall(\"goo.gl|bit.ly\", url):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "    \n",
        "    # having_At_Symbol\n",
        "    if re.findall(\"@\", url):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "    \n",
        "    # double_slash_redirecting\n",
        "    if re.findall(r\"[^https?:]//\",url):\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "    \n",
        "    # Prefix_Suffix\n",
        "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # having_Sub_Domain\n",
        "    if len(re.findall(\"\\.\", url)) == 1:\n",
        "        data_set.append(-1)\n",
        "    elif len(re.findall(\"\\.\", url)) == 2:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "    \n",
        "    # SSLfinal_State\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # Domain_registeration_length\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # Favicon\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # port\n",
        "    try:\n",
        "        port = domain.split(\":\")[1]\n",
        "        if port:\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # HTTPS_token\n",
        "    if re.findall(\"^https\\-\", domain):\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # Request_URL\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # URL_of_Anchor\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # Links_in_tags\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # SFH\n",
        "    data_set.append(0)\n",
        "\n",
        "    # Submitting_to_email\n",
        "    if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # Abnormal_URL\n",
        "    if response.text == \"\":\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # Redirect\n",
        "    if len(response.history) <= 1:\n",
        "        data_set.append(-1)\n",
        "    elif len(response.history) <= 4:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # on_mouseover\n",
        "    if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # RightClick\n",
        "    if re.findall(r\"event.button ?== ?2\", response.text):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # popUpWidnow\n",
        "    if re.findall(r\"alert\\(\", response.text):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # Iframe\n",
        "    if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # age_of_domain\n",
        "    try:\n",
        "        registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
        "        if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # DNSRecord\n",
        "    data_set.append(-1)\n",
        "\n",
        "    # web_traffic\n",
        "    try:\n",
        "        if global_rank > 0 and global_rank < 100000:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # Page_Rank\n",
        "    try:\n",
        "        if global_rank > 0 and global_rank < 100000:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # Google_Index\n",
        "    try:\n",
        "        if global_rank > 0 and global_rank < 100000:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # Links_pointing_to_page\n",
        "    number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
        "    if number_of_links == 0:\n",
        "        data_set.append(1)\n",
        "    elif number_of_links <= 2:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # Statistical_report\n",
        "    data_set.append(-1)\n",
        "\n",
        "    print (data_set)\n",
        "\n",
        "    return data_set\n",
        "\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def load_data():\n",
        "    '''\n",
        "    Load data from CSV file\n",
        "    '''\n",
        "    # Load the training data from the CSV file\n",
        "    training_data = np.genfromtxt('dataset.csv', delimiter=',', dtype=np.int32)\n",
        "\n",
        "    # Extract the inputs from the training data\n",
        "    inputs = training_data[:,:-1]\n",
        "\n",
        "    # Extract the outputs from the training data\n",
        "    outputs = training_data[:, -1]\n",
        "\n",
        "    # This model follow 80-20 rule on dataset\n",
        "    # Split 80% for traning and 20% testing\n",
        "    boundary = int(0.8*len(inputs))\n",
        "\n",
        "    training_inputs, training_outputs, testing_inputs, testing_outputs = train_test_split(inputs, outputs, test_size=0.33)\n",
        "\n",
        "    # Return the four arrays\n",
        "    return training_inputs, training_outputs, testing_inputs, testing_outputs\n",
        "\n",
        "def run(classifier, name):\n",
        "    '''\n",
        "    Run the classifier to calculate the accuracy score\n",
        "    '''\n",
        "    # Load the training data\n",
        "    train_inputs, test_inputs,train_outputs, test_outputs = load_data()\n",
        "\n",
        "    # Train the decision tree classifier\n",
        "    classifier.fit(train_inputs, train_outputs)\n",
        "\n",
        "    # Use the trained classifier to make predictions on the test data\n",
        "    predictions = classifier.predict(test_inputs)\n",
        "\n",
        "    # Print the accuracy (percentage of phishing websites correctly predicted)\n",
        "    accuracy = 100.0 * accuracy_score(test_outputs, predictions)\n",
        "    print (\"Accuracy score using {} is: {}\\n\".format(name, accuracy))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    '''\n",
        "    Main function -\n",
        "    Following are several models trained to detect phishing webstes.\n",
        "    Only the best and worst classifier outputs are displayed.\n",
        "    '''\n",
        "\n",
        "    # Decision tree\n",
        "    # classifier = tree.DecisionTreeClassifier()\n",
        "    # run(classifier, \"Decision tree\")\n",
        "\n",
        "    # Random forest classifier (low accuracy)\n",
        "    # classifier = RandomForestClassifier()\n",
        "    # run(classifier, \"Random forest\")\n",
        "\n",
        "    # Custom random forest classifier 1\n",
        "    print (\"Best classifier for detecting phishing websites.\")\n",
        "    classifier = RandomForestClassifier(n_estimators=500, max_depth=15, max_leaf_nodes=10000)\n",
        "    run(classifier, \"Random forest\")\n",
        "\n",
        "    # Linear SVC classifier\n",
        "    # classifier = svm.SVC(kernel='linear')\n",
        "    # run(classifier, \"SVC with linear kernel\")\n",
        "\n",
        "    # RBF SVC classifier\n",
        "    # classifier = svm.SVC(kernel='rbf')\n",
        "    # run(classifier, \"SVC with rbf kernel\")\n",
        "\n",
        "    # Custom SVC classifier 1\n",
        "    # classifier = svm.SVC(decision_function_shape='ovo', kernel='linear')\n",
        "    # run(classifier, \"SVC with ovo shape\")\n",
        "\n",
        "    # Custom SVC classifier 2\n",
        "    # classifier = svm.SVC(decision_function_shape='ovo', kernel='rbf')\n",
        "    # run(classifier, \"SVC with ovo shape\")\n",
        "\n",
        "    # NuSVC classifier\n",
        "    # classifier = svm.NuSVC()\n",
        "    # run(classifier, \"NuSVC\")\n",
        "\n",
        "    # OneClassSVM classifier\n",
        "    print (\"Worst classifier for detecting phishing websites.\")\n",
        "    classifier = svm.OneClassSVM()\n",
        "    run(classifier, \"One Class SVM\")\n",
        "\n",
        "    # print \"K nearest neighbours algorithm.\"\n",
        "    # nbrs = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "    # run(nbrs, \"K nearest neighbours\")\n",
        "\n",
        "    # Gradient boosting classifier\n",
        "    # classifier = GradientBoostingClassifier()\n",
        "    # run(classifier, \"GradientBoostingClassifier\")\n",
        "\n",
        "    # Take user input and check whether its phishing URL or not.\n",
        "    str=\"https://colab.research.google.com/drive/1Iynd-w-oeSLGL8CSELuTauZZlxWVxqfc\"\n",
        "    if len(str) > 1:\n",
        "        data_set = generate_data_set(str)\n",
        "\n",
        "        # Reshape the array\n",
        "        data_set = np.array(data_set).reshape(1, -1)\n",
        "\n",
        "        # Load the date\n",
        "        train_inputs, test_inputs,train_outputs, test_outputs = load_data()\n",
        "\n",
        "        # Create and train the classifier\n",
        "        classifier = RandomForestClassifier(n_estimators=500, max_depth=15, max_leaf_nodes=10000)\n",
        "        classifier.fit(train_inputs, train_outputs)\n",
        "\n",
        "        print( classifier.predict(data_set))\n",
        "        list=classifier.predict(data_set)\n",
        "        if(list[-1]==1):\n",
        "          print(\"Phishing\")\n",
        "        else:\n",
        "          print(\"Not pHishing\")"
      ]
    }
  ]
}